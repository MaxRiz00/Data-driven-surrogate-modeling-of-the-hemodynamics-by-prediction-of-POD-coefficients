{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## CROSS VALIDATION for velocity\n",
    "\n",
    "Code used for the cross validation of the neural network for the velocity. It is used to find the optimal value of the regularization parameter $\\lambda$, and assess the performance of the network.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# General setups and imports\n",
    "from utils import *\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# initialization and seeds setup: the choice of these seeds has been fixed for reproducibility\n",
    "if device==\"cuda:0\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.init()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81vEcuWCie5R",
    "outputId": "8613ba63-e27f-408d-be63-d4c634898081"
   },
   "outputs": [],
   "source": [
    "# Normalization of input parameters in [0, 1] range\n",
    "# u1 u2 u3 u4 u5 u6\n",
    "maxs = np.array([8.0, 0.3, 0.5, 0.5, 0.5, 0.0])\n",
    "mins = np.array([4.0, 0.1, -0.1, -0.5, -0.5, -0.3])\n",
    "for i in range(params.shape[1]):\n",
    "    params[:, i] = (params[:, i] - mins[i]) / (maxs[i] - mins[i])\n",
    "\n",
    "# Treat time as a parameter: add it to the parameter list\n",
    "# u1 u2 u3 u4 u5 u6 t\n",
    "times = np.linspace(0, 1, 300)\n",
    "# sample all the times for the test set\n",
    "times_test= times\n",
    "#sample all the times of training set if times[::] is written below\n",
    "#sample every 5 timesteps of the simulation if times[::5] is written below\n",
    "times_train= times[::]\n",
    "\n",
    "\n",
    "# shuffle the parameters (order of the simulations) to perform a random splitting of the dataset \n",
    "# inside the cross validation function defined below\n",
    "idx = np.random.permutation(params.shape[0])\n",
    "params = params[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Expand velocity in time through matrix multiplication\n",
    "velocity_time = solutions['velocity'] @ basis_time['velocity'].T\n",
    "\n",
    "# shuffle the velocity values, according to the order of parameters (simulations) defined above\n",
    "velocity_time = velocity_time[idx]\n",
    "\n",
    "# setting the Neural Network: all the modifications to the number of hidden layers, batch normalization layers \n",
    "# and activation functions can be implemented into \"Net\" defined below\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    # 7 input parameters, corresponding to: u1 u2 u3 u4 u5 u6 t\n",
    "    # 40 output parameters, corresponding to the POD (reduced) coefficients of the velocity solution\n",
    "    \n",
    "    # 5 hidden layers with 128 neurons each with batch normalization\n",
    "    # Tanh activation function\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.F1 = torch.nn.Tanh()\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F2 = torch.nn.Tanh()\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F3 = torch.nn.Tanh()\n",
    "        self.batch_norm3 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc4 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F4 = torch.nn.Tanh()\n",
    "        self.batch_norm4 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc5 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F5 = torch.nn.Tanh()\n",
    "        self.batch_norm5 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc6 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.F1(self.fc1(x))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.F2(self.fc2(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.F3(self.fc3(x))\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.F4(self.fc4(x))\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.F5(self.fc5(x))\n",
    "        x = self.batch_norm5(x)\n",
    "        return self.fc6(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each fold, the network is trained for `n_epochs` . The relative error, absolute error and test loss are computed. The process is repeated `k_fold` times and the mean values of the relative error, the absolute error and the test loss are computed in the end over all the folds."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# K-fold cross validation\n",
    "def cross_validation(k_indices, k, n_epoch, l1_lambda):\n",
    "    #input:\n",
    "    # k_indices: array of indices for the k-th fold\n",
    "    # k: index of the fold\n",
    "    # n_epoch: number of epochs for training\n",
    "    \n",
    "    #output:\n",
    "    #relative error, absolute error and test loss within each test fold\n",
    "    \n",
    "    #print the current k considered as test set\n",
    "    print (\"Cross validation, k = \", k)\n",
    "    #extract the indexes of the k folder for test\n",
    "    test_indices = k_indices[k]\n",
    "    #extract the remaining indexes for the training set\n",
    "    train_indices = k_indices[np.arange(len(k_indices)) != k].flatten()\n",
    "\n",
    "    # Training set\n",
    "    \n",
    "    # select the corresponding training parameters (simulations) according to the train_indices\n",
    "    params_train = params[train_indices]\n",
    "    # add time to the parameters vector\n",
    "    params_time_train = np.repeat(params_train, len(times_train), axis=0)\n",
    "    params_time_train = np.hstack((params_time_train, np.tile(times_train, len(params_train)).reshape(-1, 1)))\n",
    "    \n",
    "    #extract the corresponding velocity values for the input parameters and time\n",
    "    velocity_time_train = velocity_time[train_indices]\n",
    "    #sample the velocity values. if [:,:,::] is set, all times are considered.\n",
    "    #otherwise, with [:,:,::5] the solution is sampled every 5 time steps\n",
    "    velocity_model_train = velocity_time_train[:, :, :]\n",
    "    #reshape the training set in order to have 7 POD coefficients of velocity solution for each simulation and time\n",
    "    velocity_model_train = velocity_model_train.transpose(0, 2, 1).reshape((int(velocity_model_train.shape[0]) * len(times_train)), 40)\n",
    "\n",
    "    # Testing set\n",
    "    # select the corresponding test parameters (simulations) according to the test_indices\n",
    "    params_test = params[test_indices]\n",
    "    # add time to the parameters vector\n",
    "    params_time_test = np.repeat(params_test, len(times_test), axis=0)\n",
    "    params_time_test = np.hstack((params_time_test, np.tile(times, len(params_test)).reshape(-1, 1)))\n",
    "    #extract the corresponding velocity values for the input parameters and time (all times will be considered in this case)\n",
    "    velocity_model_test = velocity_time[test_indices]\n",
    "    #reshape the test set in order to have 7 POD coefficients of velocity solution for each simulation and time\n",
    "    velocity_model_test = velocity_model_test.transpose(0, 2, 1).reshape((int(velocity_model_test.shape[0]) * len(times_test)), 40)\n",
    "\n",
    "    # Convert to tensor\n",
    "    params_time_train = torch.tensor(params_time_train, dtype=torch.float32).to(device)\n",
    "    params_time_test = torch.tensor(params_time_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    velocity_model_train = torch.tensor(velocity_model_train, dtype=torch.float32).to(device)\n",
    "    velocity_model_test = torch.tensor(velocity_model_test, dtype=torch.float32).to(device)\n",
    "    #velocity_model_train = torch.log(torch.abs(velocity_model_train) + 1) * torch.sign(velocity_model_train)\n",
    "\n",
    "    sv_space_velocity = sv_space['velocity']\n",
    "    sv_space_velocity = sv_space_velocity / np.sum(sv_space_velocity)\n",
    "    sv_space_velocity = torch.tensor(sv_space_velocity, dtype=torch.float32).to(device)\n",
    "    sv_space_velocity = sv_space_velocity.reshape(40, 1)\n",
    "    \n",
    "    # Train the network\n",
    "    \n",
    "    #the network can be defined below choosing the input size, hidden size (same for each hidden layer) and output size\n",
    "    input_size = 7 # u1 u2 u3 u4 u5 u6 t\n",
    "    hidden_size = 128 # parameter to choose the hidden size\n",
    "    output_size = 40 # number of POD coefficients to be predicted for vellocity\n",
    "\n",
    "    net = Net(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device)\n",
    "    \n",
    "    # Define the function f, used to weight the loss function by the SVD coefficients\n",
    "    #def f(x):\n",
    "      #return torch.sqrt(x)\n",
    "\n",
    "    # Define the (weighted) loss function\n",
    "   # def loss_fn(y_p, y_true):\n",
    "   #     return torch.mean(torch.mm((y_p - y_true) ** 2, f(sv_space_velocity)))\n",
    "    \n",
    "    # Define the loss function\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    # choice of learning rate\n",
    "    learning_rate = .01\n",
    "    \n",
    "    # choice of the optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    #optimizer = torch.optim.AdamW(net.parameters(), lr=learning_rate)\n",
    "    # Set the initial learning rate and decay factor for the exponential scheduler\n",
    "\n",
    "    gamma = 0.9995  # The factor by which the learning rate will be multiplied at each epoch\n",
    "\n",
    "    # Create the ExponentialLR scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "    # Compute and print the number of weights in the model\n",
    "    if l1_lambda>0: \n",
    "        nweights = 0\n",
    "        for name,weights in net.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                nweights = nweights + weights.numel()\n",
    "        print(f'Total number of weights in the model = {nweights}')\n",
    "        \n",
    "    # initialize the losses vector\n",
    "    losses_train = []\n",
    "\n",
    "    for t in range(n_epoch):\n",
    "        #set the model to train mode\n",
    "        net.train()\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = net(params_time_train).to(device)\n",
    "\n",
    "        ## Compute the loss and put the value in the vector\n",
    "        loss_train = loss_fn(y_pred, velocity_model_train)\n",
    "        # Calculate L1 term\n",
    "        if l1_lambda>0:\n",
    "            L1_term = torch.tensor(0., requires_grad=True).to(device)\n",
    "            for name, weights in net.named_parameters():\n",
    "                if 'bias' not in name:\n",
    "                    weights_sum = torch.sum(torch.abs(weights))\n",
    "                    L1_term = L1_term + weights_sum\n",
    "            L1_term = L1_term / nweights\n",
    "            \n",
    "            # Regularize loss using L1 regularization\n",
    "            loss_train = loss_train + L1_term * l1_lambda\n",
    "\n",
    "        losses_train.append(loss_train.item())\n",
    "\n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \", t, \"Loss: \", loss_train.item())\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss_train.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step the scheduler at the end of each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Test the network\n",
    "    #predict the values for velocity\n",
    "    y_pred = net(params_time_test)\n",
    "    y_pred_numpy=y_pred.to(\"cpu\")\n",
    "    \n",
    "    # convert the output of the NN to numpy array\n",
    "    y_pred_numpy = y_pred_numpy.detach().numpy()\n",
    "    \n",
    "    #inverse of the logarithmic transformation\n",
    "    #y_pred_numpy=np.exp(y_pred_numpy)-1\n",
    "\n",
    "    # convert the velocity_model_test to numpy\n",
    "    velocity_model_test_numpy=velocity_model_test.to(\"cpu\")\n",
    "    velocity_model_test_numpy=velocity_model_test_numpy.detach().numpy()\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss_t = torch.nn.MSELoss()(y_pred, velocity_model_test)\n",
    "\n",
    "    print(\"Test loss: \", loss_t.item())\n",
    "\n",
    "    # Compute the relative error\n",
    "\n",
    "    rel_error = np.linalg.norm(y_pred_numpy - velocity_model_test_numpy, axis=1) / np.linalg.norm(velocity_model_test_numpy, axis=1)\n",
    "    \n",
    "    \n",
    "    #Compute absolute error rescaled by average values:\n",
    "    #Compute average values\n",
    "    aux = np.linalg.norm(velocity_model_test_numpy, axis=1)\n",
    "    mean_vel = np.mean(aux.reshape(-1, len(times_test)), axis=1)\n",
    "    mean_vel = np.repeat(mean_vel, len(times_test)).reshape(1, -1)\n",
    "    #rescale the absolute error by the average values within each simulation\n",
    "    abs_error = np.linalg.norm(y_pred_numpy - velocity_model_test_numpy, axis=1) / mean_vel\n",
    "    abs_error = abs_error.T\n",
    "    \n",
    "    print(\"Relative error: \", np.mean(rel_error))\n",
    "    print(\"Absolute error: \", np.mean(abs_error))\n",
    "    return np.mean(rel_error), np.mean(abs_error), loss_t.item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the cross validation for different values of the regularization parameter $\\lambda$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6iQKVRVQnzge",
    "outputId": "01349af3-640b-4045-d49e-5862399ebd23"
   },
   "outputs": [],
   "source": [
    "rel_errors = []\n",
    "abs_errors = []\n",
    "test_losses = []\n",
    "\n",
    "#define the number of folds\n",
    "k_fold = 5\n",
    "#define the size of each fold\n",
    "interval = int(params.shape[0] / k_fold)\n",
    "#extract the indexes of the parameters associated to each fold\n",
    "k_indices = np.array([idx[k * interval: (k + 1) * interval] for k in range(k_fold)])\n",
    "\n",
    "n_epoch = 2000\n",
    "rel_err = 0\n",
    "test_loss = 0\n",
    "abs_err=0\n",
    "\n",
    "# Regularization parameter L1\n",
    "l1_lambda=10**(-11) #change the value of lambda here, tested from 10^(-1) to 10^(-12)\n",
    "print(\"Lambda:\", l1_lambda)\n",
    "\n",
    "\n",
    "for k in range(k_fold):\n",
    "  rel_err, abs_err, test_loss = cross_validation(k_indices, k, n_epoch, l1_lambda)\n",
    "  rel_errors.append(rel_err)\n",
    "  abs_errors.append(abs_err)\n",
    "  test_losses.append(test_loss)\n",
    "  \n",
    "print(\"Mean relative error: \", np.mean(rel_errors))\n",
    "print(\"Mean absolute error: \", np.mean(abs_errors))\n",
    "print(\"Mean test loss: \", np.mean(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
