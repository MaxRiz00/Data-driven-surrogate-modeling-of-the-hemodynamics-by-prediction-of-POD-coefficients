{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## GRID SEARCH for velocity\n",
    "\n",
    "This code is used to find the best architecture for the NN for the velocity, we try with different number of hidden layers and different number of neurons in each layer. You can run the code for each architecture and then compare the results to find the best one.\n",
    "\n",
    "This code was run in a local machine without a GPU.\n",
    "\n",
    "Make sure the `data` folder is in the principal directory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d58bf4f1bd3bae81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General setups and imports\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "if device==\"cuda:0\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.init()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalization of input parameters in [0,1]\n",
    "maxs = np.array([8.0, 0.3, 0.5, 0.5, 0.5, 0.0])\n",
    "mins = np.array([4.0, 0.1, -0.1, -0.5, -0.5, -0.3])\n",
    "for i in range(params.shape[1]):\n",
    "    params[:, i] = (params[:, i] - mins[i]) / (maxs[i] - mins[i])\n",
    "\n",
    "\n",
    "# Shuffle the parameters\n",
    "idx = np.random.permutation(params.shape[0])\n",
    "params = params[idx]\n",
    "\n",
    "# Expand velocity in time\n",
    "vel_time = solutions['velocity'] @ basis_time['velocity'].T\n",
    "\n",
    "# Shuffle the velocity, the parameters are shuffled in the same way\n",
    "vel_time = vel_time[idx]\n",
    "\n",
    "# Split the data into training, validation and test set\n",
    "\n",
    "# Training set: 80% of the data\n",
    "# Validation set: 10% of the data\n",
    "# Test set: 10% of the data\n",
    "\n",
    "# Training set\n",
    "params_train = params[:int(0.8 * len(params))]\n",
    "vel_time_train = vel_time[:int(0.8 * len(params))]\n",
    "\n",
    "# Validation set\n",
    "params_val = params[int(0.8 * len(params)):int(0.9 * len(params))]\n",
    "vel_time_val = vel_time[int(0.8 * len(params)):int(0.9 * len(params))]\n",
    "\n",
    "\n",
    "# Test set\n",
    "params_test = params[int(0.9 * len(params)):]\n",
    "vel_time_test = vel_time[int(0.9 * len(params)):]\n",
    "\n",
    "\n",
    "# Treat time as a parameter: add it to the parameter list\n",
    "# u1 u2 u3 u4 u5 u6 t\n",
    "times = np.linspace(0, 1, 300)\n",
    "\n",
    "#sample all the times with times[:] \n",
    "#sample every 5 timesteps with times[::5] \n",
    "times_test= times[::5]\n",
    "times_train= times[::5]\n",
    "times_val= times[::5]\n",
    "\n",
    "\n",
    "# generate a matrix with parameters for each time step for training, validation and test set\n",
    "# add time as last parameter\n",
    "\n",
    "params_time_train = np.repeat(params_train, len(times_train), axis=0)\n",
    "params_time_train = np.hstack((params_time_train, np.tile(times_train, len(params_train)).reshape(-1, 1)))\n",
    "\n",
    "params_time_val = np.repeat(params_val, len(times_val), axis=0)\n",
    "params_time_val = np.hstack((params_time_val, np.tile(times_val, len(params_val)).reshape(-1, 1)))\n",
    "\n",
    "params_time_test = np.repeat(params_test, len(times_test), axis=0)\n",
    "params_time_test = np.hstack((params_time_test, np.tile(times_test, len(params_test)).reshape(-1, 1)))\n",
    "\n",
    "# if times[:] put vel_time_test[:, :, :]\n",
    "# if times[::5] put vel_time_test[:, :, ::5] \n",
    "vel_model_test= vel_time_test[:, :, ::5]\n",
    "vel_model_train= vel_time_train[:, :, ::5]\n",
    "vel_model_val= vel_time_val[:, :, ::5]\n",
    "\n",
    "# Reshape the data to have the form (number of samples, number of parameters, number of time steps)\n",
    "vel_model_train = vel_model_train.transpose(0, 2, 1).reshape((vel_model_train.shape[0] * len(times_train)), 40)\n",
    "vel_model_val = vel_model_val.transpose(0, 2, 1).reshape((vel_model_val.shape[0] * len(times_val)), 40)\n",
    "vel_model_test = vel_model_test.transpose(0, 2, 1).reshape((vel_model_test.shape[0] * len(times_test)), 40)\n",
    "\n",
    "# Take the SV coefficients of the velocity and normalize them\n",
    "sv_space_velocity = sv_space['velocity']\n",
    "sv_space_velocity = sv_space_velocity / np.sum(sv_space_velocity)\n",
    "\n",
    "\n",
    "# Convert to tensor\n",
    "params_time_train = torch.tensor(params_time_train, dtype=torch.float32).to(device)\n",
    "params_time_val = torch.tensor(params_time_val, dtype=torch.float32).to(device)\n",
    "params_time_test = torch.tensor(params_time_test, dtype=torch.float32).to(device)\n",
    "\n",
    "vel_model_train = torch.tensor(vel_model_train, dtype=torch.float32).to(device)\n",
    "vel_model_val = torch.tensor(vel_model_val, dtype=torch.float32).to(device)\n",
    "vel_model_test = torch.tensor(vel_model_test, dtype=torch.float32).to(device)\n",
    "\n",
    "sv_space_velocity = torch.tensor(sv_space_velocity, dtype=torch.float32).to(device)\n",
    "sv_space_velocity = sv_space_velocity.reshape(40, 1)\n",
    "\n",
    "\n",
    "# Possible logarithmic transformation of the data\n",
    "#vel_model_train = torch.log(torch.abs(vel_model_train) + 1) * torch.sign(vel_model_train)\n",
    "#vel_model_val = torch.log(torch.abs(vel_model_val) + 1) * torch.sign(vel_model_val)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a7f1aa657de52cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Choose the architecture of the network, run only the cell corresponding to the architecture you want to use.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96c780a5eb057a45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2 hidden layers with K neurons each with batch normalization\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.F1 = torch.nn.Tanh()\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F2 = torch.nn.Tanh()\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.F1(self.fc1(x))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.F2(self.fc2(x))\n",
    "        return self.fc3(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0c100ce160707b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3 hidden layers with K neurons each with batch normalization\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.F1 = torch.nn.Tanh()\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F2 = torch.nn.Tanh()\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F3 = torch.nn.Tanh()\n",
    "        self.fc4 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.F1(self.fc1(x))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.F2(self.fc2(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.F3(self.fc3(x))\n",
    "        return self.fc4(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fe9d89467bc0d24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4 hidden layers with K neurons each with batch normalization\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.F1 = torch.nn.Tanh()\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F2 = torch.nn.Tanh()\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F3 = torch.nn.Tanh()\n",
    "        self.batch_norm3 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc4 = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.F4 = torch.nn.Tanh()\n",
    "        self.fc5 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.F1(self.fc1(x))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.F2(self.fc2(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.F3(self.fc3(x))\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.F4(self.fc4(x))\n",
    "        return self.fc5(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49e2830b2418d5da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5 hidden layers with K neurons each with batch normalization\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.F1 = torch.nn.ReLU()\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F2 = torch.nn.Tanh()\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F3 = torch.nn.ReLU()\n",
    "        self.batch_norm3 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc4 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F4 = torch.nn.Tanh()\n",
    "        self.batch_norm4 = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.fc5 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.F5 = torch.nn.ReLU()\n",
    "        self.fc6 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.F1(self.fc1(x))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.F2(self.fc2(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.F3(self.fc3(x))\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.F4(self.fc4(x))\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.F5(self.fc5(x))\n",
    "        return self.fc6(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "720adada063d8e6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change the value of the variable `hidden_size` to change the number of neurons in the hidden layer. \n",
    "\n",
    "The number of hidden layers is determined by the architecture of the network."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab5838f0490e91e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the parameters of the network\n",
    "\n",
    "input_size = 7\n",
    "hidden_size = 128 # Change the number of neurons in the hidden layer, we tried with 32, 64, 128, 256\n",
    "output_size = 40 # POD coefficients for the velocity\n",
    "\n",
    "# Create the network\n",
    "net = Net(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device)\n",
    "\n",
    "\n",
    "# Define the loss function as the MSE loss\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "learning_rate = .01 # Starting learning rate\n",
    "\n",
    "# Use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Save the loss function for each iteration\n",
    "losses_train = []\n",
    "losses_val = []"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "582dac65f09793d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training of the network, the loss function is saved for each iteration.\n",
    "The network is trained for $2000$ epochs, you can change the number of epochs by changing the value of the variable `n_epochs`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ed4eebf8effe604"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_epochs = 2000 # Number of epochs\n",
    "\n",
    "for t in range(n_epochs):\n",
    "    net.train()\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = net(params_time_train).to(device)\n",
    "    \n",
    "    # Compute the loss.\n",
    "    loss_train = loss_fn(y_pred, vel_model_train)\n",
    "    losses_train.append(loss_train.item())\n",
    "    \n",
    "    \n",
    "    # Before the backward pass, use the optimizer object to zero all the gradients for the variables it will update (which are the learnable weights of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss_train.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    # Validation\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_val = net(params_time_val).to(device)\n",
    "        loss_val = loss_fn(y_pred_val, vel_model_val)\n",
    "        \n",
    "    if t % 100 == 0:\n",
    "        print(\"Epoch: \", t, \"Train Loss: \", loss_train.item(),\", Validation Loss: \", loss_val.item())\n",
    "        \n",
    "    losses_val.append(loss_val.item())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6f3dad7c2ae33da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test the network on the test set, the relative and absolute error are computed and printed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4738505a255c1e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Test the network\"\"\"\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = net(params_time_test).to(\"cpu\")\n",
    "\n",
    "# convert the output of the NN to numpy array\n",
    "y_pred_numpy = y_pred.detach().numpy()\n",
    "\n",
    "# convert the vel_model_test to numpy\n",
    "vel_model_test_numpy=vel_model_test.to(\"cpu\").detach().numpy()\n",
    "\n",
    "# Compute and print loss.\n",
    "loss_t = torch.nn.MSELoss()(y_pred.to(\"cpu\"), vel_model_test.to(\"cpu\"))\n",
    "\n",
    "print(\"Test loss: \", loss_t.item())\n",
    "\n",
    "# Compute the relative error\n",
    "\n",
    "rel_error = np.linalg.norm(y_pred_numpy - vel_model_test_numpy, axis=1)/np.linalg.norm(vel_model_test_numpy, axis=1)\n",
    "\n",
    "# compute the mean across the time steps for each simulation\n",
    "mean_vel=np.mean(np.linalg.norm(vel_model_test_numpy, axis=1).reshape(-1, len(times_test)), axis=1)\n",
    "# repeat the mean for every time step\n",
    "mean_vel = np.repeat(mean_vel, len(times_test)).reshape(1, -1)\n",
    "# compute the absolute error for each simulation\n",
    "abs_error = np.linalg.norm(y_pred_numpy - vel_model_test_numpy, axis=1)/mean_vel\n",
    "abs_error = abs_error.T\n",
    "\n",
    "print(\"Relative error: \", np.mean(rel_error))\n",
    "print(\"Absolute error: \", np.mean(abs_error))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff746af30a3a5710"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eccf44a7763ad347"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
